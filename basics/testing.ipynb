{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c90400e-bb71-4cd4-b279-331e6cfb9c8c",
   "metadata": {},
   "source": [
    "# Automated testing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedb6d5-5b5d-436e-83c7-31b5ee7f687d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Arrange, Act, Assert\n",
    "\n",
    "In the absense of a compelling reason to do otherwise (which is rare), each test case should test exactly one thing. Each test should clearly express what claim about the code under test it is testing, and test that claim and that claim only. Often it is feasible to name the test case with a name that will be read by humans as making that claim (e.g., `test_widgets_are_nontoxic`).\n",
    "\n",
    "A test case should:\n",
    "\n",
    "1. **Arrange.** Zero or more actions that set up for what is being tested. Some or all of these actions may be done in test *fixtures* rather than in the test cases themselves, but they are done in preparation for an individual test case.\n",
    "\n",
    "2. **Act.** An action that exercises the code under test (whose effects will be checked in the subsequent *assert* step).\n",
    "\n",
    "3. **Assert.** An assertion about the effect of the action.\n",
    "\n",
    "Arrange-Act-Assert is sometimes regarded to be incomplete. Arguably a fourth kind of test logic deserves to be distinguished explicitly:\n",
    "\n",
    "4. **Cleanup.** Zero or more actions that tear something down that was used for testing&mdash;freeing resources or restoring an invariant. Some or all of these actions may be done in test fixtures rather than in the test cases themselves, but they are done to clean up after an individual test case.\n",
    "\n",
    "These steps, where present, should always be clearly identifiable in your test code. When these steps appear within a test case, they should not be written out of order. That is, a lower-numbered step in the above list should not be written above a higher-numbered one. (In *rare* situations, it may be necessary to break this rule.)\n",
    "\n",
    "In addition to not writing them out of order, you should always strongly consider separating all these steps so that each statement does only one of those things. (You may still often need to have multiple statements that do one of them, of course.) For example, if you are going to call a function and assert something about the value it returns, you should strongly consider assigning the result to a variable, then using that variable in an assertion on a subsequent line.\n",
    "\n",
    "Controversial claim: The steps should not appear out of order, but in some situations, particularly very simple ones with a large amount of repetition, it may be defensible to combine the steps so that, for example, a statement is both acting and asserting, or both arranging and acting.\n",
    "\n",
    "- If you do this, you should always strongly consider taking the more orthodox approach of strictly separating them, understand why you are deciding not to do that, and be able to defend your decision to a hypothetical interlocutor.\n",
    "\n",
    "- Some people say this should never be done.\n",
    "\n",
    "- Do not do it if it makes tests even slightly: less clear, harder to read and understand, more complicated, or harder to verify for correctness.\n",
    "\n",
    "It should be readily apparent to anyone reading a test what logic in that test is arrangement, what logic is acting, and what logic is asserting. It is only defensible to choose to do more than one of these kinds of things in the same statement if you are confident that doing so does not make this distinction any less clear to anyone reading the code.\n",
    "\n",
    "As an opposite approach, some people write `# Arrange`, `# Act`, and `# Assert` comments identifying separate sections of their tests. This is not wrong. But I suggest against doing it, or at least against doing it *habitually*, because that information should always be readily apparent in the *code* of the test. It is possible to write complicated or unclear tests where the distinction between arrangement, acting, and asserting is not clear even if the code is separated out. If you find you're doing that, you should redesign the test, if possible. If you do separate the sections and comment them, you should make sure the distinctions would still be fully clear even without the comments.\n",
    "\n",
    "**A test case should almost always have exactly one assertion.** In particular, if you have more than one assertion, it is a strong sign you should be writing more than one test. One solution can be to parameterize the test.\n",
    "\n",
    "Subtests are a form of parameterization and, if separate assertions are done in separate subtests, then the rule to have exactly one assertion is not violated (though you should still make sure you understand why you have decided to use subtests rather than separately written tests or parameterization by some means other than subtests).\n",
    "\n",
    "Sometimes it is not feasible to avoid having multiple assertions, and not even feasible to place them in separate subtests. For example, it may be that two claims must be asserted separately for the test code to clearly express what those claims are, but that those claims are very closely tied, such that it is impossible or very misleading to test the second one unless the first one has been found to hold.  In such a situation, you may need to write a test with multiple assertions. But make sure:\n",
    "\n",
    "1. *That the first claim really is part of what you are trying to test and make an assertion about.* If instead the second claim is the one the test is really about, and the first claim is just to verify that the preconditons necessary to test the second claim have been established, then you should check for the first claim and cause the test to *error out* (rather than merely failing). The best way to do this may vary by testing framework, but often you can do it by checking for the precondition with an `if` statement and raising an exception directly. This is one of the rare situations where it it can be reasonable to raise `Exception` (rather than a more derived exception type). Make sure your exception message clearly states the reason for the error.\n",
    "\n",
    "2. *That it would actually be wrong, or at least clearly undesirable, to test the second claim if the first has failed.* If it would always be acceptable to test both, then even if you cannot reasonably write separate tests or parameterize the whole test case, you can likely use subtests (assuming your testing framework supports subtests or you have extended its functionality to do so, such as with a plugin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b19399-1756-4089-9dd0-2702154f196a",
   "metadata": {},
   "source": [
    "## Testing overlapping functionality\n",
    "\n",
    "There are three (or four) main ways of testing classes/functions with substantial overlapping functionality. In summary:\n",
    "\n",
    "0. Only test the functionality of one of them (usually bad, *sometimes* okay in inheritance).\n",
    "\n",
    "1. Parameterize the test code, typically at the level of a class that collects test-case methods.\n",
    "\n",
    "2. Use inheritance, either between test classes or from a common ABC, to share test methods (with an attribute/property for the implementation).\n",
    "\n",
    "3. Manually reproduce the shared testing logic (often excessively cumbersome; this may worsen or improve readability).\n",
    "\n",
    "Detailed explanations follow, including examples of where each of these four approaches has been used in this project.\n",
    "\n",
    "### 0. Only test the functionality on one of them.\n",
    "\n",
    "This is usually bad. It is *sometimes* okay in inheritance.\n",
    "\n",
    "When you have an abstract class that represents an interface but does not facilitate code reuse&mdash;that is, it has no concrete methods&mdash;then it is often reasonable not to write tests for it. An abstract class without any concrete methods is effectively making a *claim* about what it entails to be kind of thing. It would be hard for the tests to do anything except restate those claims more obscurely. There may occasionally be particularly important information about an interface-representing abstract class that deserves testing, though.\n",
    "\n",
    "**Other than that common situation, we've done this in `enumerations.py`**, where `BitsetEnum` does not have tests, but its derived class `Guests` does. The `Guests` tests exercise all the functionality that `Guests` inherits from `BitsetEnum`.\n",
    "\n",
    "Sometimes it is done the other way: a test is written for a base class but not for the derived class. This is much riskier because the derived class is usually in some way functionally different from the base class, and bugs in code that implements the specialized logic would not be found by testing the base class.\n",
    "\n",
    "### 1. Parameterize the test code, typically at the class level.\n",
    "\n",
    "Often the best and most straightforward way to test that claims hold true of multiple entities in code under test is to parameterize tests at the level of a class that collects test-case methods.\n",
    "\n",
    "Depending what testing framework is used and other factors, this may entail adding parameterization to an existing test class, or collecting otherwise separate methods into a class to parameterize it.\n",
    "\n",
    "**We have done this in `test_simple`** for `make_squarer` and the `MulSquarer` and `PowSquarer` subclasses of `AbstractSquarer`, demonstrating that:\n",
    "\n",
    "- Multiple test classes can be parameterized separately when some tests apply to more entities than others.\n",
    "\n",
    "- The decision of whether to inherit can be made separately in code and test. `MulSquarer` and `PowSquarer` share a base class `AbstractSquarer`, but we did not organize our tests using inheritance.\n",
    "\n",
    "### 2. Inherit shared test code.\n",
    "\n",
    "Another approach is to use inheritance, either between test classes or from a common ABC, to share test methods.\n",
    "\n",
    "One of the purposes of inheritance is code reuse, and this can be used for sharing test cases across multiple test classes, where each derived test class tests some distinct entity in the code under test.\n",
    "\n",
    "In this technique, an attribute or property, specialized in each derived class, specifies the function or class that the test should exercise. Test cases intended to be inherited then use that attribute/property rather than explicitly writing the name of the entity to test.\n",
    "\n",
    "**We have done this in `test_queues.py`** for all tests, and **in `test_simple.py`** for the *toggle* tests.\n",
    "\n",
    "When using a base class that exists only to supply test cases to derived classes, make it an ABC if you can, so if a test runner attempts to instantiate it, the failure is early and clear. Abstract or not, you must ensure test runners will not collect and attempt to run tests directly from it. How to do this varies by framework. This is a bit trickier in `unittest` than in some other frameworks like `pytest`. See \"Hiding classes from `unittest`\" below.\n",
    "\n",
    "Sometimes no abstract class is needed. If you want to test some claims about `X`, and test all those claims plus some other claims about `Y`, then `TestX` and `TestY` could both be concrete, with `TestY` inheriting from `TestX`.\n",
    "\n",
    "### 3. Manually reproduce the shared test logic.\n",
    "\n",
    "This is often excessively cumbersome. But it can be valuable in some situations.\n",
    "\n",
    "Manually reproducing the test logic may worsen or improve readability. The situations where it is reasonable to consider include:\n",
    "\n",
    "- There is very little testing logic and it is already written, so combining shared logic is not worthwhile.\n",
    "\n",
    "- Abstracting out the differences creates confusing tests. (Though sometimes this means the assumption that the code under test has overlapping functionality was not really correct.)\n",
    "\n",
    "- Abstracting out the differences creates tests that reproduce logic from the code under test. For example, separately implemented tests of `repr` are often able to simply state what the result should be, while sharing them may end up building the `repr` in a manner analogous to the code under test.\n",
    "\n",
    "   It is bad for tests to reproduce the logic under test. When they do, the same bugs will often appear both places and go undetected, and tests won't clarify or document the code.\n",
    "\n",
    "- Separating the logic facilitates writing, formatting, or commenting the test code in a way specific to the code being tested, illuminating something about the claims being made.\n",
    "\n",
    "- Manually duplicating the tests allows them to be given *names* or *docstrings* that differ in a way that clarifies something important.\n",
    "\n",
    "- It is not known to be the case that, if one of the duplicated tests must be changed, then the other(s) would need to be changed in a corresponding way. Or maybe it is known that they cannot change separately, but that knowledge is not part of what you are testing and is not related to code correctness. $1 + 2 + 3 = 6$ and $1 \\times 2 \\times 3 = 6$, but this should not be verified by a test case parameterized by operator.\n",
    "\n",
    "   Unlike the other situations listed here, this one often justifies keeping duplication *outside* tests, too. If, by coindicence, `DEFAULT_BRIGHTNESS = 76` and `TAX_ID_SUFFIX = 76`, you should still not write `DEFAULT_BRIGHTNESS = TAX_ID_SUFFIX = 76` in the code under test, and they shouldn't be tested together either.\n",
    "\n",
    "There may also be situations where it would be preferable to use some other technique, but practical considerations or organizational limitations forbid using the libraries that would be needed to use those techniques well, or forbid using some of the techniques due to the need for the tests to be understandable to novices or to engineers who primarily work in some other language.\n",
    "\n",
    "**`test_bobcats.py` duplicates shared test logic** for testing the `Bobcat` and `FierceBobcat` classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d09770-c22e-4538-8faa-5d0e27edbadf",
   "metadata": {},
   "source": [
    "## Hiding classes from `unittest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c271462b-48b6-4d39-ab6d-05cecd7bb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca218164-4d40-4a48-864d-5f346a8d0995",
   "metadata": {},
   "source": [
    "Three ways to keep a test runner of `unittest` tests from picking up test classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe8fbe-64e0-4095-963c-9dfc657f65e6",
   "metadata": {},
   "source": [
    "#### 1. Delete the names (variables) that refer to the base classes, afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082bcf7d-1f04-4720-bb25-4126b26a76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseClassForSomeTests(unittest.TestCase):\n",
    "    ...  # Blah blah, shared tests.\n",
    "\n",
    "class TestWidgets(_BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for widget tests.\n",
    "\n",
    "class TestGadgets(_BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for gadget tests.\n",
    "\n",
    "del _BaseClassForSomeTests\n",
    "# TestWidgets and TestGadgets are still derived classes of _BaseClassForSomeTests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2e750-583e-45b8-8303-750e5b3134d4",
   "metadata": {},
   "source": [
    "#### 2. Nest the base classes inside another class (created for that purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c55c548-1b85-4949-91f8-bc53b8e11003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Bases:\n",
    "    class SharedWidgetGadgetTests(unittest.TestCase):\n",
    "        ... # Blah blah, shared tests.\n",
    "\n",
    "class TestWidgets(_Bases.SharedWidgetGadgetTests):\n",
    "    ...  # Anything else we need for widget tests.\n",
    "\n",
    "class TestGadgets(_Bases.SharedWidgetGadgetTests):\n",
    "    ... # Anything else we need for gadget tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e257a9-453a-4140-a525-a83ad6a2297a",
   "metadata": {},
   "source": [
    "#### 3. Put the base classes in a separate module the test runner won't find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cdddbe-1f58-4eb1-bf96-f1adeee93828",
   "metadata": {},
   "source": [
    "We might have file `base_test_classes.py`, so the module name doesn't start with `test` (and to be safe, some test runners, in some configurations, look for `test` at the end, too, so avoid that). In that module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e9a095-8a6f-42c9-a130-0a23b6817e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassForSomeTests(unittest.TestCase):\n",
    "    ...  # Blah blah, shared tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a84b2e-7b2c-44dc-b4df-23d222831d9e",
   "metadata": {},
   "source": [
    "Then one or more other actual test modules (`test_whatever.py`) can import that module or import classes from it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331c732-b9a8-4c8a-be82-795d8b188c8a",
   "metadata": {},
   "source": [
    "```python\n",
    "import base_test_classes\n",
    "\n",
    "class TestWidgets(base_test_classes.BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for widget tests.\n",
    "    \n",
    "class TestGadgets(base_test_classes.BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for gadget tests.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069fdb9-6878-4fa2-94c7-480073ca4171",
   "metadata": {},
   "source": [
    "This approach is the least commonly done, and is mostly only reasonable when two or more separate test modules would benefit from using the base classes, or when it would otherwise be desirable to have the tests in a separate module (even if only one other module is going to use them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8b92c-cd88-4620-a139-3615119b934d",
   "metadata": {},
   "source": [
    "## Learning Check / Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9f987",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is a test fixture? Which kinds of test logic do test fixtures typically contain? How are test fixtures typically written in the `unittest` test framework? What are three places where test fixtures are used in this project, and how are they used in each? (Make sure your examples cover more than one kind of test logic achieved by fixtures.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e84a8",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Recall that sometimes you anticipate a test may fail due to a particular exception being raised, and you want to make sure the test communicates it is checking for that specific failure, with a descriptive message. Furthermore, if a test fails due to an unhandled exception, that is by default reported as the test not merely *failing*, but as having an *error*. But if the test specifically exists to anticipate and report a particular exception being raised, you may not want that considered an *error*. You've encountered and dealt with this situation. Where did you do that? What did you do?\n",
    "\n",
    "The writeup above does not cover that situation. It does, however, cover the *opposite* situation: when, to be meaningful, a test depends on something having been successfully arranged, but attempting to arrange it can fail silently, so the test checks to ensure it has succeeded. Such a check should not be done with a test assertion, because if the check fails, that represents the test having an *error*, not the test merely *failing*. (Furthermore, it should not be done with an `assert` statement, which raises `AssertionError`, as do test assertions, so those effects can be confused.) What is the technique this writeup recommends for those situations?\n",
    "\n",
    "Where is that technique used already in this project? You may not yet have written code that uses it, but as of this writing, the technique appears 13 times in the project. Make sure you find all those occurrences. (You should use tooling to find them, rather than manually reading through every test module and hoping to notice them.) Describe at least two of these checks in detail, picking two that substantially differ from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53b3ec",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Sometimes code under test has overlapping functionality; then the tests should usually have overlapping functionality, too, in order to make the same or similar assertions about the behavior of the code under test. But overlapping functionality in tests may or may not be achieved the same way as in the code under test. Even if they both use techniques to reduce or eliminate code duplication (i.e., to achieve code reuse), they may or may not be using the same techniques.\n",
    "\n",
    "Where do we have code reuse by inheritance in code under test, but test-code reuse by parameterization? Where do we have overlapping functionality in code under test that is *not* achieved by inheritance, but test-code reuse by inheritance? (In the latter case, the code under test may not use inheritance at all, or it may use inheritance but not to achieve code reuse. Recall, and make sure you understand, the other benefit inheritance may offer, besides code reuse.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3952cb",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Are there ways the bobcats tests benefit from manually reproduced test logic? If so, what are some of them? If not, or if so but you consider the benefits not to justify the reproduction, then what technique might you use to reduce or eliminate the duplicated test logic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be6f69",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "When using the nested base class technique to prevent `unittest` from collecting test cases, should the class that contains the base classes be named starting with an underscore? Why or why not? Should the base classes it contains be named starting with an underscore? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8da6a9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c90400e-bb71-4cd4-b279-331e6cfb9c8c",
   "metadata": {},
   "source": [
    "# Automated testing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedb6d5-5b5d-436e-83c7-31b5ee7f687d",
   "metadata": {},
   "source": [
    "## Arrange, Act, Assert\n",
    "\n",
    "In the absense of a compelling reason to do otherwise (which is rare), each test case should test exactly one thing. Each test should clearly express what claim about the code under test it is testing, and test that claim and that claim only. Often it is feasible to name the test case with a name that will be read by humans as making that claim (e.g., `test_widgets_are_nontoxic`).\n",
    "\n",
    "A test case should:\n",
    "\n",
    "1. **Arrange.** Zero or more actions that set up for what is being tested. Some or all of these actions may be done in test *fixtures* rather than in the test cases themselves, but they are done in preparation for an individual test case.\n",
    "\n",
    "2. **Act.** An action that exercises the code under test (whose effects will be checked in the subsequent *assert* step).\n",
    "\n",
    "3. **Assert.** An assertion about the effect of the action.\n",
    "\n",
    "Arrange-Act-Assert is sometimes regarded to be incomplete. Arguably a fourth kind of test logic deserves to be distinguished explicitly:\n",
    "\n",
    "4. **Cleanup.** Zero or more actions that tear something down that was used for testing&mdash;freeing resources or restoring an invariant. Some or all of these actions may be done in test fixtures rather than in the test cases themselves, but they are done to clean up after an individual test case.\n",
    "\n",
    "These steps, where present, should always be clearly identifiable in your test code. When these steps appear within a test case, they should not be written out of order. That is, a lower-numbered step in the above list should not be written above a higher-numbered one. (In *rare* situations, it may be necessary to break this rule.)\n",
    "\n",
    "In addition to not writing them out of order, you should always strongly consider separating all these steps so that each statement does only one of those things. (You may still often need to have multiple statements that do one of them, of course.) For example, if you are going to call a function and assert something about the value it returns, you should strongly consider assigning the result to a variable, then using that variable in an assertion on a subsequent line.\n",
    "\n",
    "Controversial claim: The steps should not appear out of order, but in some situations, particularly very simple ones with a large amount of repetition, it may be defensible to combine the steps so that, for example, a statement is both acting and asserting, or both arranging and acting.\n",
    "\n",
    "- If you do this, you should always strongly consider taking the more orthodox approach of strictly separating them, understand why you are deciding not to do that, and be able to defend your decision to a hypothetical interlocutor.\n",
    "\n",
    "- Some people say this should never be done.\n",
    "\n",
    "- Do not do it if it makes tests even slightly: less clear, harder to read and understand, more complicated, or harder to verify for correctness.\n",
    "\n",
    "It should be readily apparent to anyone reading a test what logic in that test is arrangement, what logic is acting, and what logic is asserting. It is only defensible to choose to do more than one of these kinds of things in the same statement if you are confident that doing so does not make this distinction any less clear to anyone reading the code.\n",
    "\n",
    "As an opposite approach, some people write `# Arrange`, `# Act`, and `# Assert` comments identifying separate sections of their tests. This is not wrong. But I suggest against doing it, or at least against doing it *habitually*, because that information should always be readily apparent in the *code* of the test. It is possible to write complicated or unclear tests where the distinction between arrangement, acting, and asserting is not clear even if the code is separated out. If you find you're doing that, you should redesign the test, if possible. If you do separate the sections and comment them, you should make sure the distinctions would still be fully clear even without the comments.\n",
    "\n",
    "**A test case should almost always have exactly one assertion.** In particular, if you have more than one assertion, it is a strong sign you should be writing more than one test. One solution can be to parameterize the test.\n",
    "\n",
    "Subtests are a form of parameterization and, if separate assertions are done in separate subtests, then the rule to have exactly one assertion is not violated (though you should still make sure you understand why you have decided to use subtests rather than separately written tests or parameterization by some means other than subtests).\n",
    "\n",
    "Sometimes it is not feasible to avoid having multiple assertions, and not even feasible to place them in separate subtests. For example, it may be that two claims must be asserted separately for the test code to clearly express what those claims are, but that those claims are very closely tied, such that it is impossible or very misleading to test the second one unless the first one has been found to hold.  In such a situation, you may need to write a test with multiple assertions. But make sure:\n",
    "\n",
    "1. *That the first claim really is part of what you are trying to test and make an assertion about.* If instead the second claim is the one the test is really about, and the first claim is just to verify that the preconditons necessary to test the second claim have been established, then you should check for the first claim and cause the test to *error out* (rather than merely failing). The best way to do this may vary by testing framework, but often you can do it by checking for the precondition with an `if` statement and raising an exception directly. This is one of the rare situations where it it can be reasonable to raise `Exception` (rather than a more derived exception type). Make sure your exception message clearly states the reason for the error.\n",
    "\n",
    "2. *That it would actually be wrong, or at least clearly undesirable, to test the second claim if the first has failed.* If it would always be acceptable to test both, then even if you cannot reasonably write separate tests or parameterize the whole test case, you can likely use subtests (assuming your testing framework supports subtests or you have extended its functionality to do so, such as with a plugin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b19399-1756-4089-9dd0-2702154f196a",
   "metadata": {},
   "source": [
    "## Testing overlapping functionality\n",
    "\n",
    "There are three (or four) main ways of testing classes/functions with substantial overlapping functionality. In summary:\n",
    "\n",
    "0. Only test the functionality of one of them (usually bad, *sometimes* okay in inheritance).\n",
    "\n",
    "1. Parameterize the test code, typically at the level of a class that collects test-case methods.\n",
    "\n",
    "2. Use inheritance, either between test classes or from a common ABC, to share test methods (with an attribute/property for the implementation).\n",
    "\n",
    "3. Manually reproduce the shared testing logic (often excessively cumbersome; this may worsen or improve readability).\n",
    "\n",
    "Detailed explanations follow, including examples of where each of these four approaches has been used in this project.\n",
    "\n",
    "### 0. Only test the functionality on one of them.\n",
    "\n",
    "This is usually bad. It is *sometimes* okay in inheritance.\n",
    "\n",
    "When you have an abstract class that represents an interface but does not facilitate code reuse&mdash;that is, it has no concrete methods&mdash;then it is often reasonable not to write tests for it. An abstract class without any concrete methods is effectively making a *claim* about what it entails to be kind of thing. It would be hard for the tests to do anything except restate those claims more obscurely. There may occasionally be particularly important information about an interface-representing abstract class that deserves testing, though.\n",
    "\n",
    "**Other than that common situation, we've done this in `enumerations.py`**, where `BitsetEnum` does not have tests, but its derived class `Guests` does. The `Guests` tests exercise all the functionality that `Guests` inherits from `BitsetEnum`.\n",
    "\n",
    "Sometimes it is done the other way: a test is written for a base class but not for the derived class. This is much riskier because the derived class is usually in some way functionally different from the base class, and bugs in code that implements the specialized logic would not be found by testing the base class.\n",
    "\n",
    "### 1. Parameterize the test code, typically at the class level.\n",
    "\n",
    "Often the best and most straightforward way to test that claims hold true of multiple entities in code under test is to parameterize tests at the level of a class that collects test-case methods.\n",
    "\n",
    "Depending what testing framework is used and other factors, this may entail adding parameterization to an existing test class, or collecting otherwise separate methods into a class to parameterize it.\n",
    "\n",
    "**We have done this in `test_simple`** for `make_squarer` and the `MulSquarer` and `PowSquarer` subclasses of `AbstractSquarer`, demonstrating that:\n",
    "\n",
    "- Multiple test classes can be parameterized separately when some tests apply to more entities than others.\n",
    "\n",
    "- The decision of whether to inherit can be made separately in code and test. `MulSquarer` and `PowSquarer` share a base class `AbstractSquarer`, but we did not organize our tests using inheritance.\n",
    "\n",
    "### 2. Inherit shared test code.\n",
    "\n",
    "Another approach is to use inheritance, either between test classes or from a common ABC, to share test methods.\n",
    "\n",
    "One of the purposes of inheritance is code reuse, and this can be used for sharing test cases across multiple test classes, where each derived test class tests some distinct entity in the code under test.\n",
    "\n",
    "In this technique, an attribute or property, specialized in each derived class, specifies the function or class that the test should exercise. Test cases intended to be inherited then use that attribute/property rather than explicitly writing the name of the entity to test.\n",
    "\n",
    "**We have done this in `test_queues.py`** for all tests.\n",
    "\n",
    "When using a base class that exists only to supply test cases to derived classes, make it an ABC, so if a test runner attempts to instantiate it, the failure is early and clear. Abstract or not, you must ensure test runners will not collect and attempt to run tests directly from it. How to do this varies by framework. This is a bit trickier in `unittest` than in some other frameworks like `pytest`. See \"Hiding classes from `unittest`\" below.\n",
    "\n",
    "Sometimes no abstract class is needed. If you want to test some claims about `X`, and test all those claims plus some other claims about `Y`, then `TestX` and `TestY` could both be concrete, with `TestY` inheriting from `TestX`.\n",
    "\n",
    "### 3. Manually reproduce the shared test logic.\n",
    "\n",
    "This is often excessively cumbersome. But it can be valuable in some situations.\n",
    "\n",
    "Manually reproducing the test logic may worsen or improve readability. The situations where it is reasonable to consider include:\n",
    "\n",
    "- There is very little testing logic and it is already written, so combining shared logic is not worthwhile.\n",
    "\n",
    "- Abstracting out the differences creates confusing tests. (Though sometimes this means that assumption that the code under test has overlapping functionality was not really correct.)\n",
    "\n",
    "- Astracting out the differences creates tests that reproduce logic from the code under test. For example, separately implemented tests of `repr` are often able to simply state what the result should be, while sharing them may end up building the `repr` in a manner analogous to the code under test.\n",
    "\n",
    "- Separating the logic facilitates writing, formatting, or commenting the code in a way specific to the code being tested, illuminating something about the claim being made.\n",
    "\n",
    "- Duplicating the tests manually allows them to be given *names* or *docstrings* that differ in a way that clarifies something important.\n",
    "\n",
    "- It is not known to be the case that, if one of the duplicated tests must be changed, then the other(s) would need to be changed in a corresponding way. (In other words, the situations where it is reasonable to duplicate even non-test code can also, occasionally, happen in test code.)\n",
    "\n",
    "There may also be situations where it would be preferable to use some other technique, but practical considerations or organizational limitations forbid using the libraries that would be needed to use those techniques well, or forbid using some of the techniques due to the need for the tests to be understandable to novices or to engineers who primarily work in some other language.\n",
    "\n",
    "**`test_bobcats.py` duplicates shared test logic** for testing the `Bobcat` and `FierceBobcat` classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d09770-c22e-4538-8faa-5d0e27edbadf",
   "metadata": {},
   "source": [
    "## Hiding classes from `unittest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c271462b-48b6-4d39-ab6d-05cecd7bb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca218164-4d40-4a48-864d-5f346a8d0995",
   "metadata": {},
   "source": [
    "Three ways to keep a test runner of `unittest` tests from picking up test classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe8fbe-64e0-4095-963c-9dfc657f65e6",
   "metadata": {},
   "source": [
    "#### 1. Delete the names (variables) that refers to the base classes, afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082bcf7d-1f04-4720-bb25-4126b26a76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseClassForSomeTests(unittest.TestCase):\n",
    "    ...  # Blah blah, shared tests.\n",
    "\n",
    "class TestWidgets(_BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for widget tests.\n",
    "\n",
    "class TestGadgets(_BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for gadget tests.\n",
    "\n",
    "del _BaseClassForSomeTests\n",
    "# TestWidgets and TestGadgets are still derived classes of _BaseClassForSomeTests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2e750-583e-45b8-8303-750e5b3134d4",
   "metadata": {},
   "source": [
    "#### 2. Nest the base classes inside another class (created for that purpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c55c548-1b85-4949-91f8-bc53b8e11003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Bases:\n",
    "    class SharedWidgetGadgetTests(unittest.TestCase):\n",
    "        ... # Blah blah, shared tests.\n",
    "\n",
    "class TestWidgets(_Bases.SharedWidgetGadgetTests):\n",
    "    ...  # Anything else we need for widget tests.\n",
    "\n",
    "class TestGadgets(_Bases.SharedWidgetGadgetTests):\n",
    "    ... # Anything else we need for gadget tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e257a9-453a-4140-a525-a83ad6a2297a",
   "metadata": {},
   "source": [
    "#### 3. Put the base classes in a separate module the test runner won't find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cdddbe-1f58-4eb1-bf96-f1adeee93828",
   "metadata": {},
   "source": [
    "We might have file `base_test_classes.py`, so the module name doesn't start with `test` (and to be safe, some test runners, in some configurations, look for `test` at the end, too, so avoid that). In that module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e9a095-8a6f-42c9-a130-0a23b6817e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassForSomeTests(unittest.TestCase):\n",
    "    ...  # Blah blah, shared tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a84b2e-7b2c-44dc-b4df-23d222831d9e",
   "metadata": {},
   "source": [
    "Then one or more other actual test modules (`test_whatever.py`) can import that module or import classes from it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331c732-b9a8-4c8a-be82-795d8b188c8a",
   "metadata": {},
   "source": [
    "```python\n",
    "from base_test_classes import BaseClassForSomeTests\n",
    "\n",
    "class TestWidgets(BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for widget tests.\n",
    "    \n",
    "class TestGadgets(BaseClassForSomeTests):\n",
    "    ...  # Anything else we need for gadget tests.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069fdb9-6878-4fa2-94c7-480073ca4171",
   "metadata": {},
   "source": [
    "This approach is the least commonly done, and is mostly only reasonable when two or more separate test modules would benefit from using the base classes, or when it would otherwise be desirable to have the tests in a separate module (even if only one other module is going to use them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8b92c-cd88-4620-a139-3615119b934d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
